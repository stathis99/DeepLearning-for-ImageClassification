{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ec1179-779b-4b4e-bee7-2946a8f70ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 17:42:07.606939: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 17:42:07.973549: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import layers, optimizers \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f8292b-8d36-4703-be8c-a87938282e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths \n",
    "training_dir = \"train_images_inferred_bad5/train_images\" #This is the inferred augmented dir\n",
    "testing_dir = \"test_images\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346e0280-c090-440b-981a-7fe59249ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "class_names_dict = np.load(\"class_names.npy\", allow_pickle=True).item()\n",
    "attributes_npy = np.load(\"attributes.npy\", allow_pickle=True)\n",
    "attributes_df = pd.read_csv(\"attributes.txt\", header=None, index_col = 0, sep=\" |::\", names=(\"attribute\", \"value\"), engine='python')\n",
    "training_labels_df = pd.read_csv(\"train_images.csv\", header = 0)\n",
    "test_images_paths = pd.read_csv(\"test_images_path.csv\", header = 0)\n",
    "test_images_sample = pd.read_csv(\"test_images_sample.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e81addc-e484-448f-af59-4de63e8547e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "attributes_npy_standarize = np.copy(attributes_npy)\n",
    "\n",
    "for col_idx in range(attributes_npy.shape[1]):\n",
    "\n",
    "    current_column = attributes_npy[:, col_idx]\n",
    "\n",
    "    z_scores = (current_column - np.mean(current_column)) / np.std(current_column)\n",
    "\n",
    "    threshold = 1\n",
    "    attributes_npy_standarize[:, col_idx] = np.where(z_scores > threshold, 1, 0)\n",
    "\n",
    "print(attributes_npy_standarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2822795d-fc94-436b-b7a1-8f292ceb92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the training images from the balanced augmented training dir\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "resize_dim = (224, 224)\n",
    "\n",
    "# Iterate through subdirectories\n",
    "for subdir in range(1, 201):\n",
    "    subdir_path = os.path.join(training_dir, str(subdir))\n",
    "    counter = 0\n",
    "    # Iterate through files in each subdirectory\n",
    "    for filename in os.listdir(subdir_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            \n",
    "            # Read images\n",
    "            image_path = os.path.join(subdir_path, filename)\n",
    "            img = cv2.imread(image_path)\n",
    "            \n",
    "            # Fix images\n",
    "            img_resized = cv2.resize(img, resize_dim)\n",
    "            img_normalized = img_resized / 255.0\n",
    "            \n",
    "            #Load images\n",
    "            images.append(img_normalized)\n",
    "            labels.append(subdir)\n",
    "            \n",
    "            #read only 20 images from each subfolder\n",
    "            #counter += 1\n",
    "            #if counter >= 20:\n",
    "            #    break\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images_array = np.array(images)\n",
    "labels_array = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d8a0b9-8c53-446b-b26c-2c1eb2138598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e362633-436e-42e7-a2e5-2d62e0015e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_as_labels = []\n",
    "\n",
    "# Iterate over each label and find its position in the array_of_arrays\n",
    "for label in labels_array:\n",
    "    # Subtracting 1 to match the 0-based indexing of arrays\n",
    "    position = label - 1\n",
    "    \n",
    "    # Append the corresponding array to the result_array\n",
    "    attributes_as_labels.append(attributes_npy_standarize[position])\n",
    "\n",
    "# result_array now contains 3000 arrays\n",
    "#print(attributes_as_labels)\n",
    "\n",
    "#covert to array\n",
    "attributes_as_labels = np.array(attributes_as_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea19018e-6267-4227-a477-7eb077905e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 312)\n",
      "(10000,)\n",
      "(10000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(attributes_as_labels.shape)\n",
    "print(labels_array.shape)\n",
    "print(images_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef787caf-61b3-4295-aa87-fd4f0e47c766",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdc4315f-520e-4a3f-a966-e8f89182b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to training and val set\n",
    "\n",
    "train_birds_set, val_birds_set, train_labels_set, val_labels_set = train_test_split(images_array, attributes_as_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9080a1bb-c736-42e9-873a-5ad93fc55c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 224, 224, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_birds_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06def5f1-1973-4d44-85af-fcb992ba61bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 312)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84f9cddb-3aca-419a-94b3-498dcaf73c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MODEL\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze the last 2 \n",
    "for layer in base_model.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    \n",
    "    # Add L2 regularization\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(312, activation='softmax', kernel_regularizer=l2(0.001)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a fresh instance of the model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1234e5c-2fba-48cb-b8c3-6eee1dbca8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 8s 24ms/step - loss: 0.4947 - accuracy: 0.0426 - val_loss: 0.3673 - val_accuracy: 0.0890\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3513 - accuracy: 0.0536 - val_loss: 0.3470 - val_accuracy: 0.1060\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3380 - accuracy: 0.0578 - val_loss: 0.3413 - val_accuracy: 0.0985\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3310 - accuracy: 0.0514 - val_loss: 0.3372 - val_accuracy: 0.0830\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3229 - accuracy: 0.0475 - val_loss: 0.3348 - val_accuracy: 0.0265\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3168 - accuracy: 0.0485 - val_loss: 0.3340 - val_accuracy: 0.0690\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3107 - accuracy: 0.0430 - val_loss: 0.3336 - val_accuracy: 0.0715\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3061 - accuracy: 0.0390 - val_loss: 0.3364 - val_accuracy: 0.0385\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.3007 - accuracy: 0.0371 - val_loss: 0.3308 - val_accuracy: 0.0355\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2956 - accuracy: 0.0346 - val_loss: 0.3307 - val_accuracy: 0.0605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14999f4c99c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_birds_set, train_labels_set, epochs=10, validation_data=(val_birds_set, val_labels_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d417e4-1f80-4615-b764-1fbc3546932b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
